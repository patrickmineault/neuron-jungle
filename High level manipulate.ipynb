{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-level interface\n",
    "\n",
    "Our data is in 216 blocks of 1024 x 1024 x 1024 voxels. It would be real nice to be able to manipulate the full volume using a high-level interface. We will lazily assemble a volume out of the 216 blocks and do a simple operation: we will visualize one neuron. We will find all voxels that match the target neuron id and count their numbers along the z axis using a reduction. This will give us the outline of the cell via an orthographic projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import dask\n",
    "import gcsfs\n",
    "import h5py\n",
    "import io\n",
    "import numba\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask_kubernetes import KubeCluster\n",
    "from numba.typed import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3caac3cc86df45e991f5d98b6da7c66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>KubeCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cluster = KubeCluster.from_yaml('worker-spec.yml')\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Receive client connection: Client-d6d756ac-3888-11ea-8052-26996f775df6\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    }
   ],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l4dense/neuron-volume/x1y5z2.hdf5',\n",
       " 'l4dense/neuron-volume/x5y7z2.hdf5',\n",
       " 'l4dense/neuron-volume/x1y7z2.hdf5',\n",
       " 'l4dense/neuron-volume/x4y8z3.hdf5',\n",
       " 'l4dense/neuron-volume/x5y2z1.hdf5',\n",
       " 'l4dense/neuron-volume/x4y4z3.hdf5',\n",
       " 'l4dense/neuron-volume/x1y0z2.hdf5',\n",
       " 'l4dense/neuron-volume/x5y6z0.hdf5',\n",
       " 'l4dense/neuron-volume/x4y4z1.hdf5',\n",
       " 'l4dense/neuron-volume/x5y7z0.hdf5']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../.gcs_tokens', 'rb') as f:\n",
    "    credentials = pickle.load(f)\n",
    "credentials = credentials[list(credentials.keys())[0]]\n",
    "fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "fs.ls('l4dense/neuron-volume')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Remove worker tcp://10.36.1.2:37737\n",
      "distributed.core - INFO - Removing comms to tcp://10.36.1.2:37737\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.36.2.2:36459\n",
      "distributed.core - INFO - Removing comms to tcp://10.36.2.2:36459\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.36.3.2:37939\n",
      "distributed.core - INFO - Removing comms to tcp://10.36.3.2:37939\n",
      "distributed.scheduler - INFO - Remove worker tcp://10.36.4.2:40497\n",
      "distributed.core - INFO - Removing comms to tcp://10.36.4.2:40497\n",
      "distributed.scheduler - INFO - Lost all workers\n"
     ]
    }
   ],
   "source": [
    "filename = 'test.hdf5'\n",
    "\n",
    "a = np.random.randn(100, 100)\n",
    "\n",
    "bio = io.BytesIO()\n",
    "cube = h5py.File(bio, 'w')\n",
    "cube.create_dataset('data', a.shape, compression=\"gzip\", data=a)\n",
    "cube.close()\n",
    "\n",
    "data = bio.getvalue()\n",
    "with open(f'/tmp/{filename}', 'wb') as f:\n",
    "    f.write(data)\n",
    "\n",
    "with open(f'/tmp/{filename}', 'rb') as f:\n",
    "    cube = h5py.File(f, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble the volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import xarray as xr\n",
    "\n",
    "filenames = ['x0y0z0.hdf5', 'x0y0z1.hdf5', 'x0y0z2.hdf5', 'x0y0z3.hdf5']\n",
    "\n",
    "fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "gcsmap = gcsfs.mapping.GCSMap('l4dense', gcs=fs, check=False)\n",
    "\n",
    "dask_arrays = []\n",
    "for fn in filenames[:1]:\n",
    "    fullname = f'l4dense/segmentation-volume/{fn}'\n",
    "    #print(fullname)\n",
    "    f = fs.open(fullname, 'rb')\n",
    "    g = h5py.File(f, 'r')\n",
    "    d = g['/data']\n",
    "    d = np.zeros((1024, 1024, 1024), dtype=np.uint8)\n",
    "    array = da.from_array(d, chunks=(1024, 1024, 1024))\n",
    "    dask_arrays.append(array)\n",
    "\n",
    "big_data = da.concatenate(dask_arrays, axis=2)  # concatenate arrays along axis 2\n",
    "\n",
    "my_dataarray = xr.Dataset({'neuron_labels': (['x', 'y', 'z'], big_data)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:        (x: 1024, y: 1024, z: 1024)\n",
       "Dimensions without coordinates: x, y, z\n",
       "Data variables:\n",
       "    neuron_labels  (x, y, z) uint8 dask.array&lt;chunksize=(1024, 1024, 1024), meta=np.ndarray&gt;</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:        (x: 1024, y: 1024, z: 1024)\n",
       "Dimensions without coordinates: x, y, z\n",
       "Data variables:\n",
       "    neuron_labels  (x, y, z) uint8 dask.array<chunksize=(1024, 1024, 1024), meta=np.ndarray>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'zarr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-bf0ba6a5cd32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_dataarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_zarr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgcsmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36mto_zarr\u001b[0;34m(self, store, mode, synchronizer, group, encoding, compute, consolidated, append_dim)\u001b[0m\n\u001b[1;32m   1625\u001b[0m             \u001b[0mcompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m             \u001b[0mconsolidated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconsolidated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1627\u001b[0;31m             \u001b[0mappend_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mappend_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1628\u001b[0m         )\n\u001b[1;32m   1629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mto_zarr\u001b[0;34m(dataset, store, mode, synchronizer, group, encoding, compute, consolidated, append_dim)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0msynchronizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         \u001b[0mconsolidate_on_close\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconsolidated\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     )\n\u001b[1;32m   1324\u001b[0m     \u001b[0mzstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mappend_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/backends/zarr.py\u001b[0m in \u001b[0;36mopen_group\u001b[0;34m(cls, store, mode, synchronizer, group, consolidated, consolidate_on_close)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mconsolidate_on_close\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     ):\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mzarr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mopen_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynchronizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'zarr'"
     ]
    }
   ],
   "source": [
    "my_dataarray.to_zarr(store=gcsmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 1.07 GB </td> <td> 1.07 GB </td></tr>\n",
       "    <tr><th> Shape </th><td> (1024, 1024, 1024) </td> <td> (1024, 1024, 1024) </td></tr>\n",
       "    <tr><th> Count </th><td> 1 Tasks </td><td> 1 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> uint8 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"250\" height=\"240\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"120\" x2=\"80\" y2=\"190\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"190\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.000000,0.000000 80.588235,70.588235 80.588235,190.588235 10.000000,120.000000\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"130\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"200\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"130\" y1=\"0\" x2=\"200\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.000000,0.000000 130.000000,0.000000 200.588235,70.588235 80.588235,70.588235\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"200\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"80\" y1=\"190\" x2=\"200\" y2=\"190\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"190\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"200\" y1=\"70\" x2=\"200\" y2=\"190\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"80.588235,70.588235 200.588235,70.588235 200.588235,190.588235 80.588235,190.588235\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"140.588235\" y=\"210.588235\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >1024</text>\n",
       "  <text x=\"220.588235\" y=\"130.588235\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,220.588235,130.588235)\">1024</text>\n",
       "  <text x=\"35.294118\" y=\"175.294118\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,35.294118,175.294118)\">1024</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<array, shape=(1024, 1024, 1024), dtype=uint8, chunksize=(1024, 1024, 1024), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gcsfs.core.GCSFileSystem at 0x7f3c80b8c390>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gcsfs\n",
    "\n",
    "ds = xr.Dataset()\n",
    "\n",
    "# write to the bucket\n",
    "ds.to_zarr(store=gcsmap)\n",
    "# read it back\n",
    "ds_gcs = xr.open_zarr(gcsmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum().compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
