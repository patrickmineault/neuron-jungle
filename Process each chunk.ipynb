{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import dask\n",
    "import gcsfs\n",
    "import h5py\n",
    "import io\n",
    "import numba\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask_kubernetes import KubeCluster\n",
    "from numba.typed import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Clear task state\n",
      "/opt/conda/lib/python3.7/site-packages/distributed/dashboard/core.py:72: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n",
      "distributed.scheduler - INFO -   Scheduler at:    tcp://10.36.0.28:33787\n",
      "distributed.scheduler - INFO -   dashboard at:                    :41697\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e379f904d1d5466a96cc4240a6d4ac21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>KubeCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster = KubeCluster.from_yaml('worker-spec.yml')\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Receive client connection: Client-048eb164-3871-11ea-8035-d6ea29a0b863\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    }
   ],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the client is working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def the_sum(a, b):\n",
    "    return a + b\n",
    "the_sum(the_sum(1, 2), 3).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../../.gcs_tokens'):\n",
    "    # Get a token\n",
    "    gcsfs.GCSFileSystem(project='neuron-jungle', token='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../.gcs_tokens', 'rb') as f:\n",
    "    credentials = pickle.load(f)\n",
    "credentials = credentials[list(credentials.keys())[0]]\n",
    "fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "fs.ls('l4dense/segmentation-volume')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a map from segment id to neuron id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263193540"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def locally_cache(filename, credentials):\n",
    "    fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "    with fs.open(f'l4dense/{filename}', 'rb') as f:\n",
    "        data = f.read()\n",
    "    with open(f'../cache/{filename}', 'wb') as f:\n",
    "        f.write(data)\n",
    "    return len(data)\n",
    "\n",
    "locally_cache('dendrites.hdf5', credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/dendrites/agglomerate\" (11400 members)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dendrites['dendrites']['agglomerate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrites = h5py.File('../cache/dendrites.hdf5', 'r')\n",
    "\n",
    "d = collections.defaultdict(lambda: [])\n",
    "neuronId = np.array(dendrites['dendrites']['neuronId'])\n",
    "for i, id in enumerate(neuronId):\n",
    "    if id > 0:\n",
    "        # Append the dendrite ids to the right slot.\n",
    "        d[id] += np.array(dendrites['dendrites']['agglomerate'][str(i + 1)]).tolist()\n",
    "    \n",
    "neuron_map = {}\n",
    "for neuron_id, segment_ids in d.items():\n",
    "    for segment_id in segment_ids:\n",
    "        neuron_map[segment_id] = neuron_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def remap(data, the_map):\n",
    "    b = np.zeros_like(data)\n",
    "    for i in range(len(data)):\n",
    "        if data[i] in the_map:\n",
    "            b[i] = the_map[data[i]]\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To repaint: map dendrite ids to neuron id (default to 0)\n",
    "@dask.delayed\n",
    "def repaint(filename, credentials):    \n",
    "    # Create a typed map for segment_to_neuron\n",
    "    fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "    with fs.open('l4dense/neuron-map.pkl', 'rb') as f:\n",
    "        segment_to_neuron = pickle.loads(f.read())\n",
    "\n",
    "    the_map_typed = Dict.empty(key_type=numba.int32, value_type=numba.uint8)\n",
    "    for k, v in segment_to_neuron.items():\n",
    "        the_map_typed[k] = v\n",
    "    \n",
    "    with fs.open(f'l4dense/segmentation-volume/{filename}', 'rb') as f:    \n",
    "        cube = h5py.File(f, 'r')\n",
    "        \n",
    "        a = np.zeros((1024, 1024, 1024), dtype=np.uint8)\n",
    "        \n",
    "        slice_size = 32\n",
    "        nslices = int(1024 / slice_size)\n",
    "        \n",
    "        for j in range(nslices):\n",
    "            subd = np.array(cube['data'][(slice_size*j):(slice_size*(j+1)), :, :])\n",
    "            for i in range(nslices):\n",
    "                a[i + j*slice_size, :, :] = remap(subd[i, :, :].ravel(), \n",
    "                                   the_map_typed).astype(np.uint8).reshape((1024, 1024))\n",
    "    \n",
    "    bio = io.BytesIO()\n",
    "    cube = h5py.File(bio, 'w')\n",
    "    cube.create_dataset('data', a.shape, compression=\"gzip\", data=a)\n",
    "    cube.close()\n",
    "\n",
    "    data = bio.getvalue()\n",
    "    with fs.open(f'l4dense/neuron-volume/{filename}', 'wb') as f:\n",
    "        f.write(data)\n",
    "    return len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "with fs.open('l4dense/neuron-map.pkl', 'wb') as f:\n",
    "    f.write(pickle.dumps(neuron_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x5y8z3 are the largest ids\n",
    "bytes_total = 0\n",
    "for i in range(6):\n",
    "    for j in range(9):\n",
    "        for k in range(4):\n",
    "            print(i, j, k)\n",
    "            bytes_total += repaint(f\"x{i}y{j}z{k}.hdf5\", credentials)\n",
    "bytes_total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n",
      "216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Register tcp://10.36.2.2:45201\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.36.2.2:45201\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    }
   ],
   "source": [
    "print(len(sorted(fs.glob('l4dense/segmentation-volume/*.hdf5'))))\n",
    "print(len(sorted(fs.glob('l4dense/neuron-volume/*.hdf5'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
