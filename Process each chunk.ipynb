{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import dask\n",
    "import gcsfs\n",
    "import h5py\n",
    "import io\n",
    "import numba\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask_kubernetes import KubeCluster\n",
    "from numba.typed import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.scheduler - INFO -   Scheduler at:    tcp://10.36.0.42:40839\n",
      "distributed.scheduler - INFO -   dashboard at:                     :8787\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac7d8071eef4121bcdd02f685b99a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>KubeCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster = KubeCluster.from_yaml('worker-spec.yml')\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Receive client connection: Client-29386508-394f-11ea-8077-3effd2c6d20c\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    }
   ],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the client is working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def the_sum(a, b):\n",
    "    return a + b\n",
    "the_sum(the_sum(1, 2), 3).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../../.gcs_tokens'):\n",
    "    # Get a token\n",
    "    gcsfs.GCSFileSystem(project='neuron-jungle', token='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l4dense/segmentation-volume/x3y6z0.hdf5',\n",
       " 'l4dense/segmentation-volume/x2y2z0.hdf5',\n",
       " 'l4dense/segmentation-volume/x2y7z3.hdf5',\n",
       " 'l4dense/segmentation-volume/x2y8z0.hdf5',\n",
       " 'l4dense/segmentation-volume/x1y2z0.hdf5',\n",
       " 'l4dense/segmentation-volume/x4y5z1.hdf5',\n",
       " 'l4dense/segmentation-volume/x2y7z0.hdf5',\n",
       " 'l4dense/segmentation-volume/x5y0z3.hdf5',\n",
       " 'l4dense/segmentation-volume/x0y3z0.hdf5',\n",
       " 'l4dense/segmentation-volume/x5y4z2.hdf5']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../.gcs_tokens', 'rb') as f:\n",
    "    credentials = pickle.load(f)\n",
    "credentials = credentials[list(credentials.keys())[0]]\n",
    "fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "fs.ls('l4dense/segmentation-volume')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a map from segment id to neuron id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263193540"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def locally_cache(filename, credentials):\n",
    "    fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "    with fs.open(f'l4dense/{filename}', 'rb') as f:\n",
    "        data = f.read()\n",
    "    with open(f'../cache/{filename}', 'wb') as f:\n",
    "        f.write(data)\n",
    "    return len(data)\n",
    "\n",
    "locally_cache('dendrites.hdf5', credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/dendrites/agglomerate\" (11400 members)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dendrites['dendrites']['agglomerate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrites = h5py.File('../cache/dendrites.hdf5', 'r')\n",
    "\n",
    "d = collections.defaultdict(lambda: [])\n",
    "neuronId = np.array(dendrites['dendrites']['neuronId'])\n",
    "for i, id in enumerate(neuronId):\n",
    "    if id > 0:\n",
    "        # Append the dendrite ids to the right slot.\n",
    "        d[id] += np.array(dendrites['dendrites']['agglomerate'][str(i + 1)]).tolist()\n",
    "    \n",
    "neuron_map = {}\n",
    "for neuron_id, segment_ids in d.items():\n",
    "    for segment_id in segment_ids:\n",
    "        neuron_map[segment_id] = neuron_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Remove worker tcp://10.36.9.2:39039\n",
      "distributed.core - INFO - Removing comms to tcp://10.36.9.2:39039\n"
     ]
    }
   ],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def remap(data, the_map):\n",
    "    b = np.zeros_like(data)\n",
    "    c = {}\n",
    "    for i in range(len(data)):\n",
    "        if data[i] in the_map:\n",
    "            b[i] = the_map[data[i]]\n",
    "            c[the_map[data[i]]] = 1\n",
    "    return b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To repaint: map dendrite ids to neuron id (default to 0)\n",
    "@dask.delayed\n",
    "def repaint(filename, credentials):    \n",
    "    # Create a typed map for segment_to_neuron\n",
    "    fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "    with fs.open('l4dense/neuron-map.pkl', 'rb') as f:\n",
    "        segment_to_neuron = pickle.loads(f.read())\n",
    "\n",
    "    the_map_typed = Dict.empty(key_type=numba.int32, value_type=numba.uint8)\n",
    "    for k, v in segment_to_neuron.items():\n",
    "        the_map_typed[k] = v\n",
    "    \n",
    "    neuron_ids = set()\n",
    "    with fs.open(f'l4dense/segmentation-volume/{filename}', 'rb') as f:    \n",
    "        cube = h5py.File(f, 'r')\n",
    "        \n",
    "        a = np.zeros((1024, 1024, 1024), dtype=np.uint8)\n",
    "        \n",
    "        slice_size = 32\n",
    "        nslices = int(1024 / slice_size)\n",
    "        \n",
    "        for j in range(nslices):\n",
    "            subd = np.array(cube['data'][(slice_size*j):(slice_size*(j+1)), :, :])\n",
    "            for i in range(nslices):\n",
    "                r, neuron_id = remap(subd[i, :, :].ravel(), the_map_typed)\n",
    "                a[i + j*slice_size, :, :] = r.astype(np.uint8).reshape((1024, 1024))\n",
    "                neuron_ids = neuron_ids.union(set(neuron_id.keys()))\n",
    "    \n",
    "    neuron_ids = np.array(list(neuron_ids))\n",
    "    \n",
    "    bio = io.BytesIO()\n",
    "    cube = h5py.File(bio, 'w')\n",
    "    cube.create_dataset('data', a.shape, compression=\"gzip\", data=a)\n",
    "    cube.create_dataset('neuron_ids', neuron_ids.shape, data=neuron_ids)\n",
    "    cube.close()\n",
    "\n",
    "    data = bio.getvalue()\n",
    "    with fs.open(f'l4dense/neuron-volume/{filename}', 'wb') as f:\n",
    "        f.write(data)\n",
    "    return len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "with fs.open('l4dense/neuron-map.pkl', 'wb') as f:\n",
    "    f.write(pickle.dumps(neuron_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n",
      "0 0 1\n",
      "0 0 2\n",
      "0 0 3\n",
      "0 1 0\n",
      "0 1 1\n",
      "0 1 2\n",
      "0 1 3\n",
      "0 2 0\n",
      "0 2 1\n",
      "0 2 2\n",
      "0 2 3\n",
      "0 3 0\n",
      "0 3 1\n",
      "0 3 2\n",
      "0 3 3\n",
      "0 4 0\n",
      "0 4 1\n",
      "0 4 2\n",
      "0 4 3\n",
      "0 5 0\n",
      "0 5 1\n",
      "0 5 2\n",
      "0 5 3\n",
      "0 6 0\n",
      "0 6 1\n",
      "0 6 2\n",
      "0 6 3\n",
      "0 7 0\n",
      "0 7 1\n",
      "0 7 2\n",
      "0 7 3\n",
      "0 8 0\n",
      "0 8 1\n",
      "0 8 2\n",
      "0 8 3\n",
      "1 0 0\n",
      "1 0 1\n",
      "1 0 2\n",
      "1 0 3\n",
      "1 1 0\n",
      "1 1 1\n",
      "1 1 2\n",
      "1 1 3\n",
      "1 2 0\n",
      "1 2 1\n",
      "1 2 2\n",
      "1 2 3\n",
      "1 3 0\n",
      "1 3 1\n",
      "1 3 2\n",
      "1 3 3\n",
      "1 4 0\n",
      "1 4 1\n",
      "1 4 2\n",
      "1 4 3\n",
      "1 5 0\n",
      "1 5 1\n",
      "1 5 2\n",
      "1 5 3\n",
      "1 6 0\n",
      "1 6 1\n",
      "1 6 2\n",
      "1 6 3\n",
      "1 7 0\n",
      "1 7 1\n",
      "1 7 2\n",
      "1 7 3\n",
      "1 8 0\n",
      "1 8 1\n",
      "1 8 2\n",
      "1 8 3\n",
      "2 0 0\n",
      "2 0 1\n",
      "2 0 2\n",
      "2 0 3\n",
      "2 1 0\n",
      "2 1 1\n",
      "2 1 2\n",
      "2 1 3\n",
      "2 2 0\n",
      "2 2 1\n",
      "2 2 2\n",
      "2 2 3\n",
      "2 3 0\n",
      "2 3 1\n",
      "2 3 2\n",
      "2 3 3\n",
      "2 4 0\n",
      "2 4 1\n",
      "2 4 2\n",
      "2 4 3\n",
      "2 5 0\n",
      "2 5 1\n",
      "2 5 2\n",
      "2 5 3\n",
      "2 6 0\n",
      "2 6 1\n",
      "2 6 2\n",
      "2 6 3\n",
      "2 7 0\n",
      "2 7 1\n",
      "2 7 2\n",
      "2 7 3\n",
      "2 8 0\n",
      "2 8 1\n",
      "2 8 2\n",
      "2 8 3\n",
      "3 0 0\n",
      "3 0 1\n",
      "3 0 2\n",
      "3 0 3\n",
      "3 1 0\n",
      "3 1 1\n",
      "3 1 2\n",
      "3 1 3\n",
      "3 2 0\n",
      "3 2 1\n",
      "3 2 2\n",
      "3 2 3\n",
      "3 3 0\n",
      "3 3 1\n",
      "3 3 2\n",
      "3 3 3\n",
      "3 4 0\n",
      "3 4 1\n",
      "3 4 2\n",
      "3 4 3\n",
      "3 5 0\n",
      "3 5 1\n",
      "3 5 2\n",
      "3 5 3\n",
      "3 6 0\n",
      "3 6 1\n",
      "3 6 2\n",
      "3 6 3\n",
      "3 7 0\n",
      "3 7 1\n",
      "3 7 2\n",
      "3 7 3\n",
      "3 8 0\n",
      "3 8 1\n",
      "3 8 2\n",
      "3 8 3\n",
      "4 0 0\n",
      "4 0 1\n",
      "4 0 2\n",
      "4 0 3\n",
      "4 1 0\n",
      "4 1 1\n",
      "4 1 2\n",
      "4 1 3\n",
      "4 2 0\n",
      "4 2 1\n",
      "4 2 2\n",
      "4 2 3\n",
      "4 3 0\n",
      "4 3 1\n",
      "4 3 2\n",
      "4 3 3\n",
      "4 4 0\n",
      "4 4 1\n",
      "4 4 2\n",
      "4 4 3\n",
      "4 5 0\n",
      "4 5 1\n",
      "4 5 2\n",
      "4 5 3\n",
      "4 6 0\n",
      "4 6 1\n",
      "4 6 2\n",
      "4 6 3\n",
      "4 7 0\n",
      "4 7 1\n",
      "4 7 2\n",
      "4 7 3\n",
      "4 8 0\n",
      "4 8 1\n",
      "4 8 2\n",
      "4 8 3\n",
      "5 0 0\n",
      "5 0 1\n",
      "5 0 2\n",
      "5 0 3\n",
      "5 1 0\n",
      "5 1 1\n",
      "5 1 2\n",
      "5 1 3\n",
      "5 2 0\n",
      "5 2 1\n",
      "5 2 2\n",
      "5 2 3\n",
      "5 3 0\n",
      "5 3 1\n",
      "5 3 2\n",
      "5 3 3\n",
      "5 4 0\n",
      "5 4 1\n",
      "5 4 2\n",
      "5 4 3\n",
      "5 5 0\n",
      "5 5 1\n",
      "5 5 2\n",
      "5 5 3\n",
      "5 6 0\n",
      "5 6 1\n",
      "5 6 2\n",
      "5 6 3\n",
      "5 7 0\n",
      "5 7 1\n",
      "5 7 2\n",
      "5 7 3\n",
      "5 8 0\n",
      "5 8 1\n",
      "5 8 2\n",
      "5 8 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Remove worker tcp://10.36.13.2:46263\n",
      "distributed.core - INFO - Removing comms to tcp://10.36.13.2:46263\n",
      "distributed.scheduler - INFO - Register tcp://10.36.13.2:36307\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.36.13.2:36307\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    }
   ],
   "source": [
    "# x5y8z3 are the largest ids\n",
    "bytes_total = 0\n",
    "for i in range(6):\n",
    "    for j in range(9):\n",
    "        for k in range(4):\n",
    "            print(i, j, k)\n",
    "            bytes_total += repaint(f\"x{i}y{j}z{k}.hdf5\", credentials)\n",
    "bytes_total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "with open('chunk_template.xdmf', 'r') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "with fs.open('l4dense/chunk_template.xdmf', 'w') as f:\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  7, 19, 20, 32, 33, 37, 38, 49, 53, 58, 68, 70, 79, 88, 89],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: gsutil: not found\n"
     ]
    }
   ],
   "source": [
    "!gsutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'vtk' has no attribute 'vtkXdmfReader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0ba68c7b7e5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mprocess_one_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'neuron-volume/x0y0z0.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-0ba68c7b7e5c>\u001b[0m in \u001b[0;36mprocess_one_chunk\u001b[0;34m(filename, credentials)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Prepare to read the file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mreaderVolume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvtkXdmfReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mreaderVolume\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetFileName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_xdmf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mreaderVolume\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'vtk' has no attribute 'vtkXdmfReader'"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import vtk\n",
    "\n",
    "def fetch_and_cache(filename, credentials, replacement=None):\n",
    "    fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "    \n",
    "    if replacement is not None:\n",
    "        mode = 'r'\n",
    "    else:\n",
    "        mode = 'rb'\n",
    "    \n",
    "    with fs.open(f'l4dense/{filename}', mode) as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    # Write this as a temp file.\n",
    "    _, filename = tempfile.mkstemp()\n",
    "    \n",
    "    if replacement is not None:\n",
    "        data = data.format(replacement)\n",
    "    \n",
    "    if replacement is not None:\n",
    "        mode = 'w'\n",
    "    else:\n",
    "        mode = 'wb'\n",
    "    \n",
    "    with open(filename, mode) as f:\n",
    "        f.write(data)\n",
    "    \n",
    "    return filename\n",
    "\n",
    "def process_one_chunk(filename, credentials):\n",
    "    index = 7\n",
    "    xdmf_file = \"chunk_template.xdmf\"\n",
    "    local_hdf_file = fetch_and_cache(filename, credentials)\n",
    "    local_xdmf = fetch_and_cache(xdmf_file, credentials, local_hdf_file)\n",
    "    \n",
    "    # Do the \n",
    "    colors = vtk.vtkNamedColors()\n",
    "\n",
    "    # Prepare to read the file.\n",
    "    readerVolume = vtk.vtkXdmfReader()\n",
    "    readerVolume.SetFileName(local_xdmf)\n",
    "    readerVolume.Update()\n",
    "\n",
    "    # Extract the region of interest.\n",
    "    voi = vtk.vtkExtractVOI()\n",
    "    voi.SetInputConnection(readerVolume.GetOutputPort())\n",
    "    voi.SetVOI(0, 1023, 0, 1023, 0, 1023)\n",
    "    voi.SetSampleRate(1, 1, 1)\n",
    "    voi.Update()  # Necessary for GetScalarRange().\n",
    "    srange = voi.GetOutput().GetScalarRange()  # Needs Update() before!\n",
    "    print(\"Range\", srange)\n",
    "\n",
    "    # Prepare surface generation.\n",
    "    contour = vtk.vtkDiscreteMarchingCubes()  # For label images.\n",
    "    contour.SetInputConnection(voi.GetOutputPort())\n",
    "    # contour.ComputeNormalsOn()\n",
    "\n",
    "    print(\"Doing label\", index)\n",
    "\n",
    "    contour.SetValue(0, index)\n",
    "    contour.Update()  # Needed for GetNumberOfPolys()!!!\n",
    "    \n",
    "    print(\"Done contour\")\n",
    "\n",
    "    smoother = vtk.vtkWindowedSincPolyDataFilter()\n",
    "    smoother.SetInputConnection(contour.GetOutputPort())\n",
    "    smoother.SetNumberOfIterations(20)  # This has little effect on the error!\n",
    "    smoother.BoundarySmoothingOff()\n",
    "    smoother.FeatureEdgeSmoothingOff()\n",
    "    smoother.SetPassBand(.001)        # This increases the error a lot!\n",
    "    smoother.NonManifoldSmoothingOn()\n",
    "    smoother.NormalizeCoordinatesOn()\n",
    "    smoother.GenerateErrorScalarsOn()\n",
    "    smoother.Update()\n",
    "\n",
    "    smoothed_polys = smoother.GetOutput()\n",
    "    smoother_error = smoothed_polys.GetPointData().GetScalars()\n",
    "\n",
    "    writer = vtk.vtkXMLDataSetWriter()\n",
    "    writer.SetFileName(\"out.vtp\")\n",
    "    writer.SetInputData(smoothed_polys)\n",
    "    writer.Write()\n",
    "    \n",
    "process_one_chunk('neuron-volume/x0y0z0.hdf5', credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        module\n",
       "\u001b[0;31mString form:\u001b[0m <module 'vtk' from '/opt/conda/lib/python3.7/site-packages/vtk/__init__.py'>\n",
       "\u001b[0;31mFile:\u001b[0m        /opt/conda/lib/python3.7/site-packages/vtk/__init__.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "This module loads the entire VTK library into its namespace.  It\n",
       "also allows one to use specific packages inside the vtk directory..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vtk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
