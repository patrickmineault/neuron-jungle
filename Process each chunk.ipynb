{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import dask\n",
    "import gcsfs\n",
    "import h5py\n",
    "import io\n",
    "import numba\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask_kubernetes import KubeCluster\n",
    "from numba.typed import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.scheduler - INFO -   Scheduler at:   tcp://10.36.0.105:44901\n",
      "distributed.scheduler - INFO -   dashboard at:                     :8787\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5110c8e4c0f648f6807f96b34d5ffb9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>KubeCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster = KubeCluster.from_yaml('worker-spec.yml')\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Receive client connection: Client-5c17c15a-43b7-11ea-8049-162d0c8719c5\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    }
   ],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the client is working well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dask.delayed\n",
    "def the_sum(a, b):\n",
    "    return a + b\n",
    "the_sum(the_sum(1, 2), 3).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../../.gcs_tokens'):\n",
    "    # Get a token\n",
    "    gcsfs.GCSFileSystem(project='neuron-jungle', token='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l4dense/segmentation-volume/x3y2z3.hdf5',\n",
       " 'l4dense/segmentation-volume/x4y6z2.hdf5',\n",
       " 'l4dense/segmentation-volume/x5y7z2.hdf5',\n",
       " 'l4dense/segmentation-volume/x5y1z0.hdf5',\n",
       " 'l4dense/segmentation-volume/x5y5z3.hdf5',\n",
       " 'l4dense/segmentation-volume/x0y0z0.hdf5',\n",
       " 'l4dense/segmentation-volume/x5y6z0.hdf5',\n",
       " 'l4dense/segmentation-volume/x4y5z1.hdf5',\n",
       " 'l4dense/segmentation-volume/x1y2z3.hdf5',\n",
       " 'l4dense/segmentation-volume/x3y8z0.hdf5']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Register tcp://10.36.34.3:34297\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.36.34.3:34297\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    }
   ],
   "source": [
    "with open('../.gcs_tokens', 'rb') as f:\n",
    "    credentials = pickle.load(f)\n",
    "credentials = credentials[list(credentials.keys())[0]]\n",
    "fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "fs.ls('l4dense/segmentation-volume')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a map from segment id to neuron id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318936822"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download(filename):\n",
    "    url = f\"https://l4dense2019.brain.mpg.de/webdav/{filename}\"\n",
    "    result = requests.get(url, verify=False)\n",
    "    result.raise_for_status()\n",
    "    return result.content\n",
    "\n",
    "def upload(filename, data, credentials):\n",
    "    fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "    with fs.open(f'l4dense/{filename}', 'wb') as f:\n",
    "        num_bytes = f.write(data)\n",
    "    return num_bytes\n",
    "\n",
    "def mirror(filename):\n",
    "    print(f\"Fetching {filename}\")\n",
    "    data = download(filename)\n",
    "    num_bytes = upload(filename, data, credentials)\n",
    "    return num_bytes\n",
    "\n",
    "\n",
    "def locally_cache(filename, credentials):\n",
    "    fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "    with fs.open(f'l4dense/{filename}', 'rb') as f:\n",
    "        data = f.read()\n",
    "    with open(f'../cache/{filename}', 'wb') as f:\n",
    "        f.write(data)\n",
    "    return len(data)\n",
    "\n",
    "mirror('axons.hdf5')\n",
    "locally_cache('dendrites.hdf5', credentials)\n",
    "locally_cache('axons.hdf5', credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrites = h5py.File('../cache/dendrites.hdf5', 'r')\n",
    "axons = h5py.File('../cache/axons.hdf5', 'r')\n",
    "\n",
    "# Build a map from agglomerate ID to neuron id\n",
    "agg_to_neuron_id = {k: v for k, v in zip(list(dendrites['dendrites']['agglomerate']), list(dendrites['dendrites']['neuronId']))}\n",
    "\n",
    "d = collections.defaultdict(lambda: [])\n",
    "for agg in list(dendrites['dendrites']['agglomerate'].keys()):\n",
    "    if agg in agg_to_neuron_id and agg_to_neuron_id[agg] > 0:\n",
    "        id = agg_to_neuron_id[agg]\n",
    "        d[id] += list(dendrites['dendrites']['agglomerate'][agg])\n",
    "        \n",
    "# Also add the axons for these neurons.\n",
    "in_map = 0\n",
    "for agg in list(axons['axons']['agglomerate'].keys()):\n",
    "    # Find the neuron id for this one.\n",
    "    if agg in agg_to_neuron_id:\n",
    "        in_map += 1\n",
    "    if agg in agg_to_neuron_id and agg_to_neuron_id[agg] > 0:\n",
    "        id = agg_to_neuron_id[agg]\n",
    "        d[id] += list(axons['axons']['agglomerate'][agg])\n",
    "        \n",
    "neuron_map = {}\n",
    "for neuron_id, segment_ids in d.items():\n",
    "    for segment_id in segment_ids:\n",
    "        neuron_map[segment_id] = neuron_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save it to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "with fs.open('l4dense/neuron-map-with-axons.pkl', 'wb') as f:\n",
    "    f.write(pickle.dumps(neuron_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Register tcp://10.36.42.2:43873\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.36.42.2:43873\n",
      "distributed.core - INFO - Starting established connection\n",
      "distributed.scheduler - INFO - Register tcp://10.36.43.2:46875\n",
      "distributed.scheduler - INFO - Starting worker compute stream, tcp://10.36.43.2:46875\n",
      "distributed.core - INFO - Starting established connection\n"
     ]
    }
   ],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def remap(data, the_map):\n",
    "    b = np.zeros_like(data)\n",
    "    c = {}\n",
    "    for i in range(len(data)):\n",
    "        if data[i] in the_map:\n",
    "            b[i] = the_map[data[i]]\n",
    "            c[the_map[data[i]]] = 1\n",
    "    return b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0b39ca08a35a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# To repaint: map dendrite ids to neuron id (default to 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmorphology\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrepaint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "# To repaint: map dendrite ids to neuron id (default to 0)\n",
    "from scipy.ndimage import morphology\n",
    "\n",
    "@dask.delayed\n",
    "def repaint(filename, credentials):    \n",
    "    # Create a typed map for segment_to_neuron\n",
    "    fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "    with fs.open('l4dense/neuron-map.pkl', 'rb') as f:\n",
    "        segment_to_neuron = pickle.loads(f.read())\n",
    "\n",
    "    the_map_typed = Dict.empty(key_type=numba.int32, value_type=numba.uint8)\n",
    "    for k, v in segment_to_neuron.items():\n",
    "        the_map_typed[k] = v\n",
    "    \n",
    "    neuron_ids = set()\n",
    "    with fs.open(f'l4dense/segmentation-volume/{filename}', 'rb') as f:    \n",
    "        cube = h5py.File(f, 'r')\n",
    "        \n",
    "        a = np.zeros((1024, 1024, 1024), dtype=np.uint8)\n",
    "        \n",
    "        slice_size = 32\n",
    "        nslices = int(1024 / slice_size)\n",
    "        \n",
    "        for j in range(nslices):\n",
    "            subd = np.array(cube['data'][(slice_size*j):(slice_size*(j+1)), :, :])\n",
    "            for i in range(nslices):\n",
    "                r, neuron_id = remap(subd[i, :, :].ravel(), the_map_typed)\n",
    "                a[i + j*slice_size, :, :] = r.astype(np.uint8).reshape((1024, 1024))\n",
    "                neuron_ids = neuron_ids.union(set(neuron_id.keys()))\n",
    "    \n",
    "    neuron_ids = np.array(list(neuron_ids))\n",
    "    \n",
    "    # Do some signal processing on each of the neurons\n",
    "    a_processed = np.zeros((1024, 1024, 1024), dtype=np.uint8)\n",
    "    for neuron_id in neuron_ids:\n",
    "        B = (a == neuron_id)\n",
    "        B = morphology.grey_erosion(morphology.binary_fill_holes(morphology.binary_dilation(B, size=3)), size=2)\n",
    "        a_processed[B] = neuron_id\n",
    "    \n",
    "    del a\n",
    "    \n",
    "    bio = io.BytesIO()\n",
    "    cube = h5py.File(bio, 'w')\n",
    "    cube.create_dataset('data', a_processed.shape, compression=\"gzip\", data=a_processed)\n",
    "    cube.create_dataset('neuron_ids', neuron_ids.shape, data=neuron_ids)\n",
    "    cube.close()\n",
    "\n",
    "    data = bio.getvalue()\n",
    "    with fs.open(f'l4dense/neuron-volume-with-axons/{filename}', 'wb') as f:\n",
    "        f.write(data)\n",
    "    return len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "# x5y8z3 are the largest ids\n",
    "bytes_total = 0\n",
    "for i in range(6):\n",
    "    for j in range(9):\n",
    "        for k in range(4):\n",
    "            print(i, j, k)\n",
    "            bytes_total += repaint(f\"x{i}y{j}z{k}.hdf5\", credentials)\n",
    "            break\n",
    "        break\n",
    "    break\n",
    "#bytes_total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "with open('chunk_template.xdmf', 'r') as f:\n",
    "    data = f.read()\n",
    "    \n",
    "with fs.open('l4dense/chunk_template.xdmf', 'w') as f:\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  7, 19, 20, 32, 33, 37, 38, 49, 53, 58, 68, 70, 79, 88, 89],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: gsutil: not found\n"
     ]
    }
   ],
   "source": [
    "!gsutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'vtk' has no attribute 'vtkXdmfReader'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0ba68c7b7e5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mprocess_one_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'neuron-volume/x0y0z0.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-0ba68c7b7e5c>\u001b[0m in \u001b[0;36mprocess_one_chunk\u001b[0;34m(filename, credentials)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Prepare to read the file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mreaderVolume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvtkXdmfReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mreaderVolume\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSetFileName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_xdmf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mreaderVolume\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUpdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'vtk' has no attribute 'vtkXdmfReader'"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import vtk\n",
    "\n",
    "def fetch_and_cache(filename, credentials, replacement=None):\n",
    "    fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "    \n",
    "    if replacement is not None:\n",
    "        mode = 'r'\n",
    "    else:\n",
    "        mode = 'rb'\n",
    "    \n",
    "    with fs.open(f'l4dense/{filename}', mode) as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    # Write this as a temp file.\n",
    "    _, filename = tempfile.mkstemp()\n",
    "    \n",
    "    if replacement is not None:\n",
    "        data = data.format(replacement)\n",
    "    \n",
    "    if replacement is not None:\n",
    "        mode = 'w'\n",
    "    else:\n",
    "        mode = 'wb'\n",
    "    \n",
    "    with open(filename, mode) as f:\n",
    "        f.write(data)\n",
    "    \n",
    "    return filename\n",
    "\n",
    "def process_one_chunk(filename, credentials):\n",
    "    index = 7\n",
    "    xdmf_file = \"chunk_template.xdmf\"\n",
    "    local_hdf_file = fetch_and_cache(filename, credentials)\n",
    "    local_xdmf = fetch_and_cache(xdmf_file, credentials, local_hdf_file)\n",
    "    \n",
    "    # Do the \n",
    "    colors = vtk.vtkNamedColors()\n",
    "\n",
    "    # Prepare to read the file.\n",
    "    readerVolume = vtk.vtkXdmfReader()\n",
    "    readerVolume.SetFileName(local_xdmf)\n",
    "    readerVolume.Update()\n",
    "\n",
    "    # Extract the region of interest.\n",
    "    voi = vtk.vtkExtractVOI()\n",
    "    voi.SetInputConnection(readerVolume.GetOutputPort())\n",
    "    voi.SetVOI(0, 1023, 0, 1023, 0, 1023)\n",
    "    voi.SetSampleRate(1, 1, 1)\n",
    "    voi.Update()  # Necessary for GetScalarRange().\n",
    "    srange = voi.GetOutput().GetScalarRange()  # Needs Update() before!\n",
    "    print(\"Range\", srange)\n",
    "\n",
    "    # Prepare surface generation.\n",
    "    contour = vtk.vtkDiscreteMarchingCubes()  # For label images.\n",
    "    contour.SetInputConnection(voi.GetOutputPort())\n",
    "    # contour.ComputeNormalsOn()\n",
    "\n",
    "    print(\"Doing label\", index)\n",
    "\n",
    "    contour.SetValue(0, index)\n",
    "    contour.Update()  # Needed for GetNumberOfPolys()!!!\n",
    "    \n",
    "    print(\"Done contour\")\n",
    "\n",
    "    smoother = vtk.vtkWindowedSincPolyDataFilter()\n",
    "    smoother.SetInputConnection(contour.GetOutputPort())\n",
    "    smoother.SetNumberOfIterations(20)  # This has little effect on the error!\n",
    "    smoother.BoundarySmoothingOff()\n",
    "    smoother.FeatureEdgeSmoothingOff()\n",
    "    smoother.SetPassBand(.001)        # This increases the error a lot!\n",
    "    smoother.NonManifoldSmoothingOn()\n",
    "    smoother.NormalizeCoordinatesOn()\n",
    "    smoother.GenerateErrorScalarsOn()\n",
    "    smoother.Update()\n",
    "\n",
    "    smoothed_polys = smoother.GetOutput()\n",
    "    smoother_error = smoothed_polys.GetPointData().GetScalars()\n",
    "\n",
    "    writer = vtk.vtkXMLDataSetWriter()\n",
    "    writer.SetFileName(\"out.vtp\")\n",
    "    writer.SetInputData(smoothed_polys)\n",
    "    writer.Write()\n",
    "    \n",
    "process_one_chunk('neuron-volume/x0y0z0.hdf5', credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        module\n",
       "\u001b[0;31mString form:\u001b[0m <module 'vtk' from '/opt/conda/lib/python3.7/site-packages/vtk/__init__.py'>\n",
       "\u001b[0;31mFile:\u001b[0m        /opt/conda/lib/python3.7/site-packages/vtk/__init__.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "This module loads the entire VTK library into its namespace.  It\n",
       "also allows one to use specific packages inside the vtk directory..\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vtk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
