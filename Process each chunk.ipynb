{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import dask\n",
    "import gcsfs\n",
    "import h5py\n",
    "import io\n",
    "import numba\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "from dask.distributed import Client\n",
    "from dask_kubernetes import KubeCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.scheduler - INFO - Clear task state\n",
      "distributed.scheduler - INFO -   Scheduler at:    tcp://10.36.0.20:46277\n",
      "distributed.scheduler - INFO -   dashboard at:                     :8787\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f135362012d4f21836febafa33dbc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>KubeCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    .â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster = KubeCluster.from_yaml('worker-spec.yml')\n",
    "#cluster.scale_up(10)  # specify number of nodes explicitly\n",
    "\n",
    "#cluster.adapt(minimum=1, maximum=100)  # or dynamically scale based on current workload\n",
    "\n",
    "#client = Client()\n",
    "#client\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../../.gcs_tokens'):\n",
    "    # Get a token\n",
    "    gcsfs.GCSFileSystem(project='neuron-jungle', token='browser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l4dense/dendrites.hdf5',\n",
       " 'l4dense/segmentation-volume/',\n",
       " 'l4dense/segments.hdf5']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../.gcs_tokens', 'rb') as f:\n",
    "    credentials = pickle.load(f)\n",
    "credentials = credentials[list(credentials.keys())[0]]\n",
    "fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "#fs.ls('l4dense/neuron')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a map from segment id to neuron id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263193540"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def locally_cache(filename, credentials):\n",
    "    fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "    with fs.open(f'l4dense/{filename}', 'rb') as f:\n",
    "        data = f.read()\n",
    "    with open(f'../cache/{filename}', 'wb') as f:\n",
    "        f.write(data)\n",
    "    return len(data)\n",
    "\n",
    "locally_cache('dendrites.hdf5', credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/dendrites/agglomerate\" (11400 members)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dendrites['dendrites']['agglomerate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrites = h5py.File('../cache/dendrites.hdf5', 'r')\n",
    "\n",
    "d = collections.defaultdict(lambda: [])\n",
    "neuronId = np.array(dendrites['dendrites']['neuronId'])\n",
    "for i, id in enumerate(neuronId):\n",
    "    if id > 0:\n",
    "        # Append the dendrite ids to the right slot.\n",
    "        d[id] += np.array(dendrites['dendrites']['agglomerate'][str(i + 1)]).tolist()\n",
    "    \n",
    "neuron_map = {}\n",
    "for neuron_id, segment_ids in d.items():\n",
    "    for segment_id in segment_ids:\n",
    "        neuron_map[segment_id] = neuron_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def remap(data, the_map):\n",
    "    b = np.zeros_like(data)\n",
    "    for i in range(len(data)):\n",
    "        if data[i] in the_map:\n",
    "            b[i] = the_map[data[i]]\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To repaint: map dendrite ids to neuron id (default to 0)\n",
    "\n",
    "\n",
    "def repaint(filename, credentials, segment_to_neuron):    \n",
    "    # Create a typed map for segment_to_neuron\n",
    "    the_map_typed = Dict.empty(key_type=numba.int32, value_type=numba.uint8)\n",
    "    for k, v in segment_to_neuron.items():\n",
    "        the_map_typed[k] = v\n",
    "        \n",
    "    fs = gcsfs.GCSFileSystem(project='neuron-jungle', token=credentials)\n",
    "    with fs.open(f'l4dense/segmentation-volume/{filename}', 'rb') as f:    \n",
    "        cube = h5py.File(f, 'r')\n",
    "        \n",
    "        a = np.zeros((1024, 1024, 1024), dtype=np.uint8)\n",
    "        \n",
    "        slice_size = 32\n",
    "        nslices = int(1024 / slice_size)\n",
    "        \n",
    "        for j in range(nslices):\n",
    "            subd = np.array(cube['data'][(slice_size*j):(slice_size*(j+1)), :, :])\n",
    "            for i in range(nslices):\n",
    "                a[i + j*slice_size, :, :] = remap(subd[i, :, :].ravel(), \n",
    "                                   the_map_typed).astype(np.uint8).reshape((1024, 1024))\n",
    "    \n",
    "    bio = io.BytesIO()\n",
    "    cube = h5py.File(bio, 'w')\n",
    "    cube.create_dataset('data', a.shape, compression=\"gzip\", data=a)\n",
    "    cube.close()\n",
    "\n",
    "    with fs.open(f'l4dense/neuron-volume/{filename}', 'wb') as f:\n",
    "        f.write(bio.read())\n",
    "    return a.max()\n",
    "\n",
    "changed = repaint(\"x0y0z0.hdf5\", credentials, segment_to_neuron)\n",
    "changed.compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
